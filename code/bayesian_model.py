import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

import numpy as np
import pandas as pd

from scipy.stats import spearmanr

"""
the probability of a learner's belief that the underlying utility function is U, 
given observed demo d is:
P_L(U | d) \propto P(d | U) * P(U)

How do we get P(d | U)?
Intuitively, P(d | U) should capture how "good" demo d is at conveying utility function U.
A demo consists of a set of objects X, and the target object selected t. It follows that
P(d | U) = P(t | X, U) \propto exp(U(X)^alpha)
"""

def compute_demo_score(all_possible_games,
                        is_first_agent, 
                        reward_matrices,
                        game,
                        alpha=1):
    """
    Given the set of all possible demos that could be shown the student, compute the 
    score associated with each demo

    Arguments:
        all_possible_games (torch.Tensor): 
            size (batch_size, 560, num_choices_in_listener_context, obj_encoding_len)
        is_first_agent (Bool):
            True if first agent in the chain, false otherwise
        reward_matrices (torch.Tensor):
            size (batch_size, num_colors*num_shapes, num_colors + num_shapes + 1)
        game (SignalingBanditsGame):
            need this to call the game.compute_reward function
        alpha (int):
            temperature parameter for the softmax

    Return:
        demo_scores (torch.Tensor):
            size (batch_size, 560)
    """
    batch_size = all_possible_games.shape[0]
    num_games = all_possible_games.shape[1]

    rewards_only = game.compute_rewards(all_possible_games, reward_matrices)    # (batch_size, num_games, 3)

    max_vals, _ = torch.max(rewards_only, dim=-1)
    numerator = torch.exp(alpha * max_vals)
    denominator = torch.sum(torch.exp(alpha * rewards_only), dim=-1)

    demo_scores = numerator / denominator
    demo_scores = F.softmax(demo_scores, dim=-1)
    
    return demo_scores


def compute_correlation(neural_game_scores, bayesian_demo_scores):
    """
    Compute the strength of correlation between the scores over all possible games generated 
    by the neural model, and the scores over all possible demos generated by the Bayesian model
    """
    flattened_neural_game_scores = torch.flatten(neural_game_scores).detach().cpu().numpy()
    flattened_bayesian_demo_scores = torch.flatten(bayesian_demo_scores).detach().cpu().numpy()

    corr, p_value = spearmanr(flattened_neural_game_scores, flattened_bayesian_demo_scores)

    if np.isnan(corr): breakpoint()
    
    return corr


def compute_mean_bayesian_score(neural_game_scores, bayesian_demo_scores):
    """
    Return the Bayesian model score associated with the highest scoring demo
    as indicated by the neural model. The goal here is to determine whether the neural model's
    prediction for the best demo aligns at all with the Bayesian model
    """
    batch_size = neural_game_scores.shape[0]
    idx = torch.argmax(neural_game_scores, dim=-1)
    bayesian_demo_scores_indexed = bayesian_demo_scores[range(batch_size), idx]
    mean = torch.mean(bayesian_demo_scores_indexed)
    #breakpoint()
    return mean.item()




